# Assignment Six 

```{r library-06, echo=FALSE,warning=FALSE, message=FALSE, comment=FALSE}

#Textbook Packages
library(fpp2)
library(AppliedPredictiveModeling)
library(mlbench)

#Processing
library(tidyverse)

#Graphing
library(ggplot2)
library(grid)
library(gridExtra)
library(lemon)

#Math
library(caret)
library(forecast)
library(randomForest)
library(seasonal)
library(psych)
library(corrplot)

#Formatting
library(knitr)
library(kableExtra)
library(default)
```

```{r settings-06, echo=F}
# Set default augments for code chunks
knitr::opts_chunk$set(echo = T, message=F, warning=F, error=F, comment=F, fig.width=10, fig.height = 3)

# Set default for ggplot theme
default(theme) <- list(axis.text.x = element_text(angle = 90, hjust = 1),
                       axis.title.x = element_blank(),
                       axis.title.y = element_blank(),
                       plot.title = element_text(color="#3d248a", size=10, face="bold"),
                       plot.subtitle = (element_text(size=8, color="#000000")),
                       legend.title = (element_text(size=10, color="#000000", face="bold")),
                       strip.background = element_rect(color="#000000", 
                                                       fill="#D3D3D3", size=1, linetype="solid"),
                       strip.text.x = element_text(size = 8, color = "#000000", face="bold"))

# wrap text
if(!knitr:::is_latex_output())
{
  options("width"=60)
  knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60, indent = 2), tidy = TRUE)
  knitr::opts_chunk$set(fig.pos = 'H')
}
```

*  Hyndman 8.1 
*  Hyndman 8.2 
*  Hyndman 8.6 
*  Hyndman 8.8

## Hyndman 8.1 

### a. Explain the differences among these figures. Do they all indicate that the data are white noise?

ACF plots (also known as correlograms) are used to graph the autocorrelation coeffients of lag values in a timeseries. In white noise series, we expect autocorrelation to be close to zero. This component is observed in the ACF plots by examining the dashed blue lines. 

Using this knowledge, we can determine that all three series are white noise because they all show just a small spread between these blue lines and hover around 0. Only two lag lines come close to exceeding this boundary, but none are significant or escape the denoted critical value used to determine autocorrelation. 

### b. Why are the critical values at different distances from the mean of zero? Why are the autocorrelations different in each figure when they each refer to white noise?

The blue line boundary on an ACF plot is calculated using the formula $\pm 2 / \sqrt{T-d}$, where $T$ represents the length of time in a series and $d$ the number of differences. Thus as $T$ grows larger, the denominator also gets larger, making the resulting quotient smaller. 

This means that the areas defined by the boundaries  $\pm 2 / \sqrt{T-d}$ decrease as the number values in series $T$ increases.

## Hyndman 8.2 

A classic example of a non-stationary series is the daily closing IBM stock price series (data set `ibmclose`). Use R to plot the daily closing prices for IBM stock and the ACF and PACF. Explain how each plot shows that the series is non-stationary and should be differenced. 

### a. Time Series Plot

```{r 8.2a}
p1 <- autoplot(ibmclose) + 
  labs(title="Non-Stationary", y="Price", x="Days")+
  theme_bw()+theme()
p2 <- autoplot(diff(ibmclose)) + 
  labs(title="Differenced", y="Price", x="Days")+
  theme_bw()+theme()

grid.arrange(p1, p2, ncol=2, top="Daily Closing IBM Stock Price")
```

Daily closing prices from the `ibmclose` dataset are non-stationary and show an overall trend component. Seasonality is harder to distinguish at this frequency. There may be a cyclical component where pattern changes occur over a non-specific period of time, however, there are not enough observations to determine this. 

We can compare this plot to a differenced timeseries, in which the stock prices are stationary and do not depend on the time at which the series is observed. Stationary models are required for autoregression modeling and analysis. 

### b. ACF Plot

```{r 8.2b}
p1 <- ggAcf(ibmclose)+theme_bw()+theme()
p2 <- ggAcf(diff(ibmclose))+theme_bw()+theme()

grid.arrange(p1, p2, ncol=2, top="Daily Closing IBM Stock Price")
```
In our `ibmclose` series, we can identify large, positive, and slowly decreasing lags. This further indicates that our data is non-stationary, trended, and autocorrelated. Seasonal fluxuations do not appear present within our lags. 

Autoregressive models, however, require stationary data. The `ibmclose` data could be differenced to meet this requirement.  The `diff(ibmclose)` series has no signs of autocorrelation outside the 95% limit ($\pm 0.1$) as shown by the dashed blue lines. This plot appears similiar to a white noise series.  

### c. PACF Plot

```{r 8.2c}
p1 <- ggPacf(ibmclose)+
    ggtitle('`ibmclose Series ACF')+theme_bw()+theme()
p2 <- ggPacf(diff(ibmclose)) +
    ggtitle('`ibmclose` ACF Differenced Series' )+theme_bw()+theme()

grid.arrange(p1, p2, ncol=2, top="Daily Closing IBM Stock Price")
```

Lastly, the PACF plot is used to look at partial autocorrelation. This plot analyzes the correlation of residuals after removing the effect explained by previous lags.  Our Pacf plot of the `ibmclose` series a sharp cut off after the first lag.

Pacf is very beneficial in determining order of moving averages component of a non-seasonal ARIMA model. In the differentiated data, there are two spikes outside the 95% limit. These can be ignored though as they do not occur in our initial lags and fall just outside the $\pm 0.1$ limit. The differentiated Pacf plot further suggests a white-noise ARIMA model of the stationary data. 

## Hyndman 8.6 

Use R to simulate and plot some data from simple ARIMA models. 

### a. Use the following R code to generate data from an AR(1) model with $\phi_1=0.6$ and $\sigma^2=1$. The process starts with $y_1=0$.

```{r 8.6a, error=T, comment=F}
# Sample Code 
y <- ts(numeric(100))
e <- rnorm(100)
for(i in 2:100)
  y[i] <- 0.6*y[i-1] + e[i]

# Function

AR1 <- function(phi1,sigma2, c){
  # add warnings for expected constraints for AR1 parameters values 
  if (phi1 < -1) warning("'phi1' should be > -1 for AR(1) models")
  if (phi1 > 1) warning("'phi1' should be < 1 for AR(1) models")
  if (phi1 < 1 & phi1 >-1) message("Model: AR(1) ")
  
  # set random number generation (RNG) state 
  set.seed(5)
  
  # generate time series 1:100
  y <- ts(numeric(100))
  
  #  RNG of epsilon with constant variance (sigma^2)
  e <- rnorm(100, sd=sigma2)
  
  # apply formula to all but the first ts observation so that y1=0
  for(i in 2:100){
    y[i] <- c+phi1*y[i-1] + e[i]
  }
  return(y)
}

# Set parameters per instruction
phi1=0.6; sigma2=1; c=0

# Generate Data
AR1(phi1, sigma2, c)
```

### b. Produce a time plot for the series. How does the plot change as you change $\phi_1$?

```{r 8.6b}

nam <- expression(phi[1])

AR1_phi1 <- suppressMessages(cbind("-0.9" = AR1(-0.9, 1,0),
                 "-0.6" = AR1(-0.6, 1,0),
                 "0.0" = AR1(0.0, 1,0), 
                 "0.6" = AR1(0.6, 1,0),
                 "0.9" = AR1(0.9, 1,0)))

pallete = c("#dfb1c5", '#86d1f1', "#000000", '#5ead7f', '#a931a1')
#palette <- brewer.pal(n = 5 , name = 'RdYlGn')
palette <-c ("#D7191C", "#C51B8A", "#555555","#756BB1", "#2C7BB6")
autoplot(AR1_phi1, size=.25, alpha=.9)+
  scale_color_manual(name = nam, values=pallete)+
  labs(title="Time Plot of AR(1) Model Series", 
       subtitle=expression("Display of Changes in"~phi~"when c=0"),
       y="Value") +
    theme_bw()+
  theme(legend.position = "bottom")
```  

*  The AR(1) Model at $\phi=0$ should be the equivalent of white noise.   
*  The $y_t$ is expected to oscillate around the mean when $\phi<0$.   
*  We would expect $y_t$ to resemble a random walk when $\phi=1$ and $c=0$.    
 
As expected we observe these patterns in the time series plots above. The variation and pattern observed in $y$ changes as the $\phi$ parameter changes with the negative values of $\phi$ occilating around the mean, and the positive numbers becoming more like a random walk as $\phi$ approaches 1.

### c. Write your own code to generate data from an MA(1) model with $\theta_1=0.6$ and $sigma^2=1$.

```{r 8.6c}
# Function
MA1 <- function(theta1,sigma2,c){
  # add warnings for expected constraints for MA1 parameters values 
  if (theta1 < 0) warning("'theta' should be > 0 for MA(1) models")
  if (theta1 > 1) warning("'theta' should be < 1 for MA(1) models")
  if (theta1 < 1 & theta1 >0) message("Model: MA(1)")
  
  # set random number generation (RNG) state 
  set.seed(5)
  
  # generate time series 1:100
  y <- ts(numeric(100))
  
  #  RNG of epsilon with constant variance (sigma^2)
  e <- rnorm(100, sd=sigma2)
  
  # apply formula to all but the first ts observation so that y1=0
  for(i in 2:100){
    y[i] <- c+theta1*e[i-1] + e[i]
  }
  return(y)
}

# Set parameters per instruction
theta1=0.6; sigma2=1; c=0

# Generate Data
MA1(theta1, sigma2, c)
```

### d. Produce a time plot for the series. How does the plot change as you change $\theta_1$?

```{r 8.6d}
nam <- expression(theta[1])

MA1_theta1 <- cbind("0.1" = suppressMessages(MA1(0.1,1,0)), 
              "0.3" = suppressMessages(MA1(0.3,1,0)),
              "0.6" = suppressMessages(MA1(0.6,1,0)),
              "0.9" = suppressMessages(MA1(0.9,1,0)))

pallete = c("#d74887", "#000000", "#86d1f1", "#5ead7f")

autoplot(suppressMessages(MA1_theta1), size=.5, alpha=.8) +
  scale_color_manual(name=nam, values=pallete) +
  labs(title="Time Plot of MA(1) Model Series", 
       subtitle=expression("Display of Changes in"~theta[1]~"when c=0 and"~sigma^2~"=1"),
       y="Value")  +
  theme_bw()+theme(legend.position = "bottom")
```

 - When $\theta_1$ increases, we observe higher peaks and troughs in the $y_t$ value.  

 - In a MA(1) model, we expect the weight of the moving average to be the most constant when $\theta_1$ nears the value of 1. 

 - Conversely, the most recent observations have more influence on $y_t$ than past observations when $\theta_1<1$.


### e. Generate data from an ARMA(1,1) model with $\phi_1=0.6$, $\theta_1=0.6$, and $\sigma^2=1$.

```{r 8.6e}
# Reference ARMA(1,1) Equation:  http://feldman.faculty.pstat.ucsb.edu/174-03/lectures/l7.pdf

# Function
ARMA11 <- function(phi1, theta1, sigma2, c){
  # add warnings for expected constraints for ARMA(1,1) parameters values 
  if (phi1 + theta1 == 0) warning("'phi1 + theta1' should be != 0 for ARMA(1,1) models ")
  if (phi1 + theta1 != 0) message("Model: ARMA(1,1) ")
  # set random number generation (RNG) state 
  set.seed(5)
  # generate time series 1:100
  y <- ts(numeric(100))
  #  RNG of epsilon with constant variance (sigma^2)
  e <- rnorm(100, sd=sigma2)
  # apply formula to all but the first ts observation so that y1=0
  for(t in 2:100){
    y[t] <- phi1*y[t-1]+e[t]+theta1*e[t-1]
  }
  return(y)
}

# Set parameters per instruction
phi1=0.6; theta1=0.6; sigma2=1; c=0

# Generate Data
e <- ARMA11(phi1, theta1, sigma2, c)
e
```

### f. Generate data from an AR(2) model with $\phi_1=-0.8$, $\phi_2=0.3$, and $\sigma^2=1$. (Note that these parameters will give a non-stationary series.)

```{r 8.6f1}
# Function
AR2 <- function(phi1, phi2, sigma2, c){
  # add warnings for expected constraints for AR2 parameters values 
  if (phi2 < -1) warning("Warning: 'phi2' should be > -1 for AR(2) model. ")
  if (phi2 > 1) warning("Warning: 'phi2' should be < 1 for AR(2) model. ")
  if (phi1 + phi2 > 1) warning("Warning: 'phi1+phi2' should be < 1 for AR(2) model. ")
  if (phi2 - phi1 > 1) warning("Warning: 'phi2-phi1' should be < 1 for AR(2) model. ")
  if (phi2 > -1 & phi1 < 1 & phi1 + phi2 < 1 & phi2 - phi1 < 1) message("Model: AR(2). ")
  # set random number generation (RNG) state 
  set.seed(5)
  # generate time series 1:100
  y <- ts(numeric(100))
  #  RNG of epsilon with constant variance (sigma^2)
  e <- rnorm(100, sd=sigma2)
  # apply formula to all but the first ts observation so that y1=0
  for(i in 2:100){
    y[i] <- c+phi1*y[i-1] + phi2*y[i-1]+ e[i]
  }
  return(y)
}

# Set parameters per instruction
phi1=-0.8; phi2=0.3; sigma2=1; c=0

# Generate Data
f <- AR2(phi1, phi2, sigma2, c)
f
```

### g. Graph the latter two series and compare them.

```{r 8.6g, message=F}
autoplot(suppressMessages(e), series="AR(1,1)", size=.5, alpha=.8)+
  autolayer(suppressMessages(f), series="AR(2)")+
  labs(title="Time Plot of AR(1,1) and AR(2) Model Series",
       y="Value")  +
  theme_bw()+theme(legend.position = "bottom") 
```   

Assumptions:  
*  When $|\theta_1|$ nears 0 in an ARMA(1,1) model, the data follows a pattern similiar to an AR(1) model.   
*  When $|\phi_1|$ nears zero, it behaves similarly to a MA(1) model. 

Our ARMA(1,1) model from  8.6(e). has parameters of $\phi_1=0.6$ and $\theta_1=0.6$, thus we do not expect explict AR(1) or MA(1) patterns.  

Our AR(2) series shows some evidence of trend. This is likely due to the parameters of the AR(2) model from 8.6(f) being set to generate non-staionary data. 

Conversely, the parameters in the ARMA(1, 1) model from 8.6(e) generate stationary.  

## Hyndman 8.8 

Consider `austa`, the total international visitors to Australia (in millions) for the period 1980-2015.  

### a. Use auto.arima() to find an appropriate ARIMA model. What model was selected. Check that the residuals look like white noise. Plot forecasts for the next 10 periods.

```{r ha8.8a}
data(austa)
(fit <-auto.arima(austa))
    residuals(fit)%>%
    checkresiduals()
    
fit%>%forecast(h=10) %>% autoplot()+theme_bw()+theme()
```  

The results of `auto.arima()` suggest single-differencing and applying an MA(1) model with the equation $y_t = c + y_{t-1} -.3006\epsilon_{t-1} \epsilon_t$

### b. Plot forecasts from an ARIMA(0,1,1) model with no drift and compare these to part a. Remove the MA term and plot again.  

```{r 8.8b}
(fit_ma <- Arima(austa, order = c(0,1,1), include.mean = FALSE))
fit_ma%>%
    autoplot()+theme_bw()+theme()
```

```{r ha8.8b2}
(fit_noma <- Arima(austa, order = c(0,1,0), include.mean = FALSE))
fit_noma%>%
    autoplot()+theme_bw()+theme()

```

### With MA  
```{r ha8.8b3}
forecast(fit_ma, h=10)%>%autoplot()  +theme_bw()+theme()
```  

### Without  MA  
```{r ha8.8b4}
forecast(fit_noma, h=10)%>%autoplot()+theme_bw()+theme()
```  

### Comparing 
```{r}
nm<-forecast(fit_noma, h=10)
accuracy(nm)
m<-forecast(fit_ma, h=10)
accuracy(m)
difference<-m$mean[1] - nm$mean[1]
```  

**Mean of MA:** `r m$mean[1]` (the forecast value for the MA)    
**Mean of Non-MA:** `r nm$mean[1]` (the forecast value for the Non-MA)    
**Mean Differences:** `r difference`    

The two models are quite similar: their forecasts differ by .16, with approximately the same confidence windows (shifted given the different means). With a better RMSE, the (0,1,1) model may be prefereable.

### (c.) Plot forecasts from an ARIMA(2,1,3) model with drift. 

```{r 8.8c1}
(fit_arima <- Arima(austa, order = c(2,1,3), include.drift = TRUE))
fit_arima%>%
    autoplot() +theme_bw()+theme()
fit_arima%>%
  forecast(h=10)%>%
  autoplot() +theme_bw()+theme()
accuracy(forecast(fit_arima))
```

Visually this model seems to improve the forecsts. It takes into account the upwasrd trend missing in the (0,1,0) and (0,1,1) forecasts; it offers a much tighter confidence window throughout the prediction range; and, lastly, its RMSE of .14 suggests a better forecast.

> Remove the constant and see what happens

```{r 8.8c2, eval= FALSE}
(fit_arima <- Arima(austa, order = c(2,1,3),  include.constant = FALSE))
fit_arima%>%
    autoplot()
```

In the current state, the coefficients for the AR component breed non-stationarity, such that the model throws an error. 

`Error in stats::arima(x = x, order = order, seasonal = seasonal, include.mean = include.mean, : non-stationary AR part from CSS`

This can be corrected by switching to the traditional "Maximum Likelihood" method or "CSS" method.

```{r 8.8c3}
(fit_arima <- Arima(austa, order = c(2,1,3),  include.constant = FALSE, method='ML'))
fit_arima%>%
    autoplot()+theme_bw()+theme()

fit_arima%>%
  forecast(h=10)%>%
  autoplot()+theme_bw()+theme()

accuracy(forecast(fit_arima))
```

This approach leads to a much wider confidence interval and unit roots moved to the extremes of the acceptable stationary range (-1,0) and (1,0) for both the MA and AR components (suggesting only marginal stationarity).

Including the constant term does seem to improve model performance and yield a more compact prediction interval.

In order to compare apples to apples, we also used the 'ML' method on the model when we add the drift.

```{r ha8.8c4}
(fit_arima <- Arima(austa, order = c(2,1,3),  include.drift = TRUE, method='ML'))
fit_arima%>%
    autoplot()+theme_bw()+theme()

fit_arima%>%
  forecast(h=10)%>%
  autoplot()+theme_bw()+theme()

accuracy(forecast(fit_arima))
```

This forecast is very similar to the original (2,1,3) model, but it compares more favorably in terms of  training error metrics. The AICc is considerably lower with constant (and drift) than without, as are the forecasting RMSE and resulting prediction interval. In short, it makes sense to allow for drift.  

### (d.) Plot forecasts from an ARIMA(0,0,1) model with a constant. Remove the MA term and plot again.  
```{r ha8.8d1}
(fit_ma <- Arima(austa, order = c(0,0,1), include.mean = TRUE))
fit_ma%>%
    autoplot()+theme_bw()+theme()

(fit_noma <- Arima(austa, order = c(0,0,0), include.mean = FALSE))
fit_noma%>%
    autoplot()+theme_bw()+theme()
```  

### With MA  
```{r ha8.8d2}
forecast(fit_ma, h=10)%>%autoplot() +theme_bw()+theme()
```  

### Without  MA  

```{r ha8.8d3}
forecast(fit_noma, h=10)%>%autoplot()+theme_bw()+theme()
```  

Even without examination of error measures its evident that these forecasts are not particularly useful. The moving average forecast begins accounting for the trend, but then deteriorates to the mean; the (0,0,0) model simply predicts a static mean over an obviously more complex time series than the forecasts reflect. 

### Accuracy
```{r 8.8d4}
accuracy(forecast(fit_ma, h=10))
accuracy(forecast(fit_noma, h=10))

```  

This assessment gains further support given the high RMSE for the MA(1) and even higher RMSE for the (0,0,0) model. Neither are particularly useful forecasts.

### (e.) Plot forecasts from an ARIMA(0,2,1) model with no constant.  
```{r ha8.8e}
(fit_ma <- Arima(austa, order = c(0,2,1), include.mean = TRUE))
fit_ma%>%
    autoplot()+theme_bw()+theme()

forecast(fit_ma, h=10)%>%autoplot()+theme_bw()+theme()

accuracy(forecast(fit_ma, h=10))

```

Based on RMSE alone, this would seem a go-to model when compared with the (0,0,1), (0,0,0) and (0,1,1) models. It is close to as good as the (2,1,3) without drift, which had an RMSE of .17; however, the (2,1,3) with drift had the overall best RMSE of .14. 

Given that the (2,1,3) model also had a tighter confidence window it makes sense to consider that there is some autoregression in this series and use the full ARIMA(2,1,3) with drift in predicting over this data set.