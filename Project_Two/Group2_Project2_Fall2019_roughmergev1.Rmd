---
title: 'DATA 624: Project 2'
author:
- Vinicio Haro
- Sang Yoon (Andy) Hwang
- Julian McEachern
- Jeremy O'Brien
- Bethany Poulin
date: '10 December 2019'
output: 
  pdf_document:
    includes:
      in_header: preamble.tex
    latex_engine: xelatex
    keep_tex: yes
    toc: true
    number_sections: no
documentclass: report
subparagraph: yes
---

File for final submission of Project 2. 

```{r formatting, echo = F, message=F, warning=F, error=F, comment=NA}
source('https://raw.githubusercontent.com/JeremyOBrien16/CUNY_DATA_624/master/Project_Two/defaults.R')
# source('~/GitHub/CUNY_DATA_624/Project_Two/defaults.R')
```

```{r load-source, echo = F, message=F, warning=F, error=F, comment=NA, results='hide', cache=T}
source('https://raw.githubusercontent.com/JeremyOBrien16/CUNY_DATA_624/master/Project_Two/script.R')
# source('~/GitHub/CUNY_DATA_624/Project_Two/script.R')
```

```{r}
library(tidyverse)
library(readxl)
library(psych)
library(ggplot2)
library(mice)
library(xtable)
library(GGally)
library(ggstance)
library(grid)
library(gridExtra)
library(ggpubr)
library(caret)
library(data.table)
library(recipes)
library(Metrics)
```

\thispagestyle{empty}
\newpage
\clearpage
\pagenumbering{arabic} 


# Introduction {-#intro}

This project is designed to evaluate production data from a beverage manufacturing company. Our assignment is to predict `PH`, a Key Performance Indicator (KPI), with a high degree of accuracy through predictive modeling. After thorough examination, we approached this task by splitting the provided data into training and test sets. We evaluated several models on this split and found that **what-ever-worked-best** method yielded the best results. 

Each group member worked individually to create their own solution. We built our final submission by collaboratively evaluating and combining each others' approaches. Our introduction should further outline individual responsibilities. For example, **so-and-so** was responsible for **xyz task**. 

For replication and grading purposes, we made our code avaliable in the appendix section. This code, along with the provided data, score-set results, and individual contributions, can also be accessed through our group github repository: 
\begin{compactitem}
  \item \href{https://github.com/JeremyOBrien16/CUNY_DATA_624/tree/master/Project_Two}{Pretend I'm a working link to R Source Code}
  \item \href{https://github.com/JeremyOBrien16/CUNY_DATA_624/tree/master/Project_Two}{Pretend I'm a working link to Provided Data}
  \item \href{https://github.com/JeremyOBrien16/CUNY_DATA_624/tree/master/Project_Two}{Pretend I'm a working link to Excel Results}
  \item \href{https://github.com/JeremyOBrien16/CUNY_DATA_624/tree/master/Project_Two}{Pretend I'm a working link to Individual Work}
\end{compactitem}


# Data Exploration 

[COMBINE BELOW FROM JULIANN...]

The beverage manufacturing production dataset contained 33 columns/variables and 2,571 rows/cases. In our initial review, we found that the response variable, `PH`, had four missing observations. 

We also identified that 94% of the predictor variables had missing data points. Despite this high occurance, the NA values in the majority of these predictors accounted for less than 1% of the total observations. Only eleven variables were missing more than 1% of data.

[...WITH BELOW FROM JEREMY]

The beverage dataset includes 2,571 cases, 32 predictor variables, and a single response variable.  One of these predictor variables  (Brand Code) is categorical with four levels - A through D; for the purpose of our analysis we have interpreted these to represent four distinct beverage brands. 

While we found missing observations in both reponse and predictor variables, in our assessment the extent of NAs did not suggest a systemic issue in measurement or recording that imputation could not remedy. 

The response variable (PH) is missing a total of four observations (< 1%).  Most (30) predictor variables are missing at least one observation, but only eleven are missing more than 1% of total cases and only three are missing more than 2% of total cases.  These are: MFR (continuous, 8.2%), BrandCode (categorical, 4.7%), and FillerSpeed (continuous, 2.2%).

```{r}
Tbl_Top_MissingData <- MissingData %>% 
  top_n(11, n) %>%
  column_to_rownames("predictor") %>% 
  kable(caption = "Variables with Highest Frequency of NA Values", 
        booktabs = T, 
        digits = 1) %>% 
  kable_styling() %>% 
  row_spec() 

Tbl_Top_MissingData
```


## Response Variable

```{r, fig.height=5, fig.cap="Distribution of Response Variable: pH", out.width = "1\\textwidth",  fig.align="right", wrapfigure = list("r", .5)}

Plt_pH1 <- StudentData %>% select(PH) %>% mutate(type="pH") %>% ggplot(aes(PH)) + geom_histogram(aes(y=..density..), bins=40, fill="#57A0D3", alpha=.65)+geom_density(alpha=.2, color="#000000",size=.65)+scale_x_continuous() + scale_y_continuous(limits = c(0,3.5)) + labs(x="",y="")+theme_bw()+theme(axis.title.x = element_blank(), axis.ticks.length.x = unit(0, "cm"))+facet_wrap(~type)
Plt_pH2 <- StudentData %>% select(PH) %>% mutate(type="pH") %>% ggplot(aes(PH,"")) + geom_boxploth(fill="#57A0D3", outlier.colour="#4682B4",alpha=.65)+ theme_bw()+theme(legend.title = element_blank(), strip.background = element_blank(), strip.text.x = element_blank(),axis.title.x = element_blank(), axis.ticks.length.x = unit(0, "cm"))+labs(x="",y="")+ scale_y_discrete(labels = 'pH')+ scale_x_continuous() + facet_wrap(~type,nrow=1, strip.position = "top")
Plt_pH3 <- StudentData %>% filter(!is.na(BrandCode)) %>% ggplot(aes(PH,"")) + geom_boxploth(aes(fill = BrandCode),outlier.colour="#4682B4",alpha=.3) + scale_fill_manual()+theme_bw()+theme(legend.position = "none", strip.background = element_blank(),strip.text.x = element_blank(),axis.title.x = element_blank(), axis.text.y = element_blank())+ labs(x="", y="")+ scale_x_continuous() + facet_wrap(~BrandCode, nrow=1, strip.position = "top", scales = 'fixed')
Plt_pH4 <- StudentData %>% select(PH, BrandCode) %>% filter(!is.na(BrandCode)) %>% ggplot(aes(PH)) + geom_histogram(aes(y=..density..,fill=BrandCode), bins=20, alpha=.65)+geom_density(alpha=.2, color="#000000",size=.65)+scale_fill_manual()+scale_x_continuous() + scale_y_continuous(limits = c(0,3.5)) + labs(x="",y="")+facet_wrap(~BrandCode, nrow=1)+theme_bw()+theme(axis.title.x = element_blank(),axis.ticks.length.x = unit(0, "cm"), axis.text.y = element_blank(), legend.position = "none")
Plt_pH_lay <- rbind(c(1,2,2), c(3,4,4))

grid.arrange(Plt_pH1, Plt_pH4, Plt_pH2, Plt_pH3, layout_matrix = Plt_pH_lay, heights=c(2,1), padding=unit(0, 'cm'))
```

[COMBINE BELOW FROM JULIANN...]

Understanding the influence pH has on our predictors is key to building an accurate predictive model. pH is a measure of acidity/alkalinity that must conform in a critical range. The value of pH ranges from 0 to 14, where 0 is acidic, 7 is neutral, and 14 is basic. 

Figure 1.1 shows that our response distribution follows a somewhat normal pattern and is centered around 8.5. The histogram for `pH` is bimodal in the aggregate, but varies by brand. The boxplot view allows us to better visualize the effect outliers have on the skewness within our target variable. 

Brand A has a negatively skewed, multimodal distribution, which could be suggestive of several distinct underlying response patterns or a higher degree of variation in `pH` response for this brand. The density plot and histogram for Brand B show two bimodal peaks with a slight positive skew.  These peaks indicate that this brand has two distinct response values that occur more frequently. The distribution for Brand C and D are both more normal, with a slight negative skew. Brand D has the highest median `pH` value and Brand C has the lowest. Brand C also appears to have the largest spread of `pH` values.  

[...WITH BELOW FROM JEREMY]

The response variable PH is a logarithmically scaled measure of how acidic or basic a water-based solution is (https://en.wikipedia.org/wiki/PH).  It ranges from 0 (acidic) and to 14 (alkaline); 7 is neutral (e.g. room temperature water).

In aggregate, PH distribution is approximately normal and centered around 8.546 (i.e. slightly base), with some negative skew / outliers. 
When evaluated by BrandCode:
- A (293 observations) appears to be multimodal and have the most outliers, with a mean slightly lower than the aggregate (8.495)
- B (1293 observations) appears to be bimodal with a number of outliers, as well as a mean nearest the aggregate (8.562)
- C (304 observations) appears to be bimodal and is the most acid (8.419)
- D (615 observations) is the most normal distribution and also has the highest alkalinity (8.603)


## Predictor Variables

We examined the density of our variables to visualize the distribution of the predictors.  Many of these variables contain outliers and present with a skewed distribution. The outliers fall outside the red-line boundaries, and highlight which predictors have heavier tails.

The density plots also contain an overlay of the only categorical indicator, `BrandCode`. This view shows us that some variables, including `AlchRel`, `CarbRel`, `CarbVolume`, `HydPressure4`, and  `Tempature`, are strongly influenced by brand type.  

```{r, fig.height=5}
Plt_Outlier1 <- ggplot(outlier_with, aes(value)) + geom_density(aes(fill=BrandCode), color="#999999", alpha=.3) + labs(title="Density Distribution of Numeric Predictor Variables", subtitle="With Outliers", x="", y="")+ geom_vline(data = outlier_with, mapping = aes(xintercept=outlier_lower), color="#ff8080")+geom_vline(data = outlier_with, mapping = aes(xintercept=outlier_upper),  color="#ff8080")+facet_wrap(~key, scales = 'free', nrow = 3)+theme_bw()+theme(axis.text.y = element_blank(), axis.title.x=element_blank(),axis.text.x = element_blank(), legend.position = "none")+scale_fill_manual()
Plt_Outlier2 <- ggplot(outlier_wo, aes(value)) + geom_density(aes(fill=BrandCode), color="#999999", alpha=.3)+ labs(subtitle="Without Outliers", x="", y="")+ facet_wrap(~key, scales = 'free', nrow = 1)+ theme_bw()+theme(axis.text.y = element_blank(),axis.title.x=element_blank(),axis.text.x = element_blank(), legend.position = "bottom", legend.key.size = unit(.4, "cm"))+scale_fill_manual()

grid.arrange(Plt_Outlier1, Plt_Outlier2, nrow=2, heights=c(3,2))
```

[COMBINE BELOW FROM JULIANN...]

We also looked at the relationship of our predictors against the response variable below. There are a few predictors that have a weak, linear association with our response variable. However, most of the indicators show no strong patterns. Given these trends, we do not expect linear modeling to provide optimal predictions for `pH`.  

This view helps us further visualize the effect `BrandCode` has on our predictor and `pH` values. For example, `AlchRel` shows distinct `BrandCode` groupings. Other variables, such as `PSCO2`, `BowlSetpoint`, `MinFlow`, and `PressureSetup` show unique features likely related to system processes. 

[...WITH BELOW FROM JEREMY]

As no predictor variable shows a particularly pronounced monotonic linear relationship with response, a non-linear approach to modeling seems warranted. 

```{r, fig.height=5}
Plt_Scatter1 <- outlier_with %>% mutate(PH=as.numeric(as.character(PH))) %>% ggplot(aes(x=value, y=PH)) + geom_jitter(aes(color=BrandCode), alpha=.15) + stat_smooth(color="#000000", method = "loess") + labs(title="pH~Predictor Scatterplots", subtitle="Without Outliers", x="", y="")+ facet_wrap(~key, scales = 'free_x', nrow = 3) + theme_bw() + theme(axis.title.x=element_blank(),axis.text.x = element_blank(), legend.position = "none", legend.key.size = unit(.4, "cm"))+scale_color_manual(values=c("#999999", "#95C8D8", "#008081", "#034D92"))
Plt_Scatter2 <- outlier_wo %>% mutate(PH=as.numeric(as.character(PH))) %>% ggplot(aes(x=value, y=PH)) + geom_jitter(aes(color=BrandCode), alpha=.15) + stat_smooth(color="#000000", method = "loess") + labs(subtitle="Without Outliers", x="", y="")+ facet_wrap(~key, scales = 'free_x', nrow = 1)+ theme_bw()+theme(axis.title.x=element_blank(),axis.text.x = element_blank(), legend.position = "bottom", legend.key.size = unit(.4, "cm"))+scale_color_manual(values=c("#999999", "#95C8D8", "#008081", "#034D92"))

grid.arrange(Plt_Scatter1, Plt_Scatter2, nrow=2, heights=c(3,2))
```

Lastly, we examined collinearity measures between our numeric predictors and found that several of these variables were heavily related, with correlation values exceeding $\pm{0.7}$. 

```{r, fig.height = 3.5, fig.width=10}
Plt_Corr <- StudentData %>% select_if(is.numeric)%>%  select(-PH)%>% ggcorr(method='pairwise.complete.obs', geom = "tile", label = F, hjust = .95, layout.exp = 7,  label_round =1, low="#95C8D8", mid="grey90",high="#034D92") +  theme_bw()+ theme(legend.key.size = unit(.6, "cm"), legend.justification=c(.05,.98), legend.position=c(.05,.98))+labs(title="Predictor Variable Correlation Matrix")
g = ggplot_build(Plt_Corr); g$data[[2]]$size = 2

grid::grid.draw(ggplot_gtable(g))
```


# Data Preparation

In our exploration, we detected missing data, extreme outliers, and multicollinearity. We kept these factors in mind and applied strategic transformations when preparing our models to evaluate their performance with and without normalization changes.


**Train/Test Splits:**   

[BASED ON FINAL ALIGNMENT, COMBINE BELOW FROM JULIANN...]

We divided the production dataset using an 80/20 split to create a train and test set. All models incorporated k-folds cross-validation set at 10 folds to protect against overfitting the data. We set up unique model tuning grids to find the optimal parameters for each regression type to ensure the highest accuracy within our predictions.

[...WITH BELOW FROM BETHANY]

Instead of doing a train-test-split, we used repeated cross validation, with 6 model repetitions and 10 folds of cross validation per model. This allows us to train over all of the observation enhancing resolution for brands with less observations.

# Models {-#mods}
For both KNN and SVM models a grid of seeds was created from our original seed to ensure that out repeated cross validation would be repeateable. The same seeds were used in both the SVM and KNN.  


**Data Imputation:**  

[BASED ON FINAL ALIGNMENT, COMBINE BELOW FROM JULIANN...]

We applied a Multiple Imputation by Chained Equations (MICE) algorthim to predict the missing data using sequential regression. This method filled in all incomplete cases, including `BrandCode`, our one unordered categorical variable.  

[...WITH BELOW FROM BETHANY]

Because the data is missing values in both the training and test sets, it is important to maintain a consistent imputational method between the Student data and the test data so that identical observations in both sets would recieve the same imputed missing values. The bigger issue in imputation is how to manage the missing `Brand Code` observations. 

It could be useful to impute them based on the other variables, but it may also be meaningful to leave consider them unknown, allowing the know classes to differentiate well, as they seem to be highly related to the outcome variable. 

Because the models under evaluation both require, scaling, centering and are not robust with categorical variables, it makes sense to use carets knn imputation for this as it will allow us to apply this method to the test data. The outcome variable, `PH` had four missing observations, those rows were simply dropped.

Test data is imputed with this model and `PH` is removed from the set.


**Pre-Processing:**  

[BASED ON FINAL ALIGNMENT, COMBINE BELOW FROM JULIANN...]

Due to the strong non-normality exhibited in the data, we tested our models using three different approaches: (1) No pre-processing techniques, (2) centering and Scaling, and (3) removing zero (and near-zero) variance and box-cox, centering, and scaling transformations.

[...WITH BELOW FROM BETHANY]

Both K-nearest neighbors and Support Vector machine Regressions require centering and scaling to reduce the bias of magnitude between variables, we also removed the variable `Hyd Pressure1` due to near zero variance and applied Box-Cox transformations to the models to satify the near-normalcy requirements.

Because neither model is robust to categorical data, the `Brand Code` was converted to dummy variables with no category for the unknowns. This seemed like the most effective way of sussing out the true influence of categories without introducing bias of imputation.


# Modeling

We assessed the effectiveness of nine different non-linear models, of which the following four performed best (performance results from the other five are included in [TABLE BLAH BLAH] in [APPENDIX BLAH BLAH]).

- Model 1: Support Vector Machines Regression
- Model 2: Cubist Tree Regression
- Model 3: Multivariate Adaptive Regression Splines Regression
- Model 4: Random Forest Regression


## Model Selection Considerations

[PULL FROM BELOW FROM ANDY AS GROUP FEELS APPROPRIATE]

As we expected, there are many outliers in `Temperature` and `Oxygen Filler`. Note that `MFR` greatly suffers from the presence of outliers.

From the presence of multiple outliers, we then have to explain why/what/how we decided to add `Random Forest` as an additional model and apply necessary pre-processing before modeling `PLS` and `Bagged Tree`, if needed. If number of outlier

Althought tree-based models, including`Bagged Tree`, are generally robust to outliers and multicollinearity, it does not necessarily mean Model A in tree-based is always equally good as another model in tree-based. Ensembled model such as `Random Forest` is potentially even more robust to outliers and multicollinearity than `Bagged Tree` since only a subset of features are selected at random out of the total and the best split feature from the subset is used to split each node in a tree, unlike in bagging where all features are considered for splitting a node. This makes `Random Forest` a potentially good alternative for `Bagged Tree`. 

Partial least squares regression (PLS regression) is used as an alternative for ordinary least squares regression in the presence of multicollinearity. Although `PLS` is quite sensitive to outliers, like in `OLS`, small number of outliers will not necessarily worsen the predictive ability. In fact, depending on the situations, removing outlier will rather decrease `MAPE` on test set. Since the benefit of removing outlier cannot be predicted in our case, we will compare `MAPE` on test set between `PLS` model without outliers and `PLS` model without handling outliers to select the best model.

Reference:

(https://www.hindawi.com/journals/jam/2018/7696302/ - "PLS regression is sensitive to outliers and leverages. Thus several robust versions have been proposed in the literature, but only for linear PLS. Hubert [7] proposed two robust versions of the SIMPLS algorithm by using a robust estimation for the variance-covariance matrix. Kondylis and Hadi [8] used the BACON algorithm to eliminate outliers, resulting in a robust linear PLS."
)


## Model 1: Support Vector Machines (SVM)

The support vector machine, although less efficient than the k-nearest neighbor to train, provided a much more robust final model using a radial kernel with a cost of 10, passed as the tune length settling on  $\sigma =  0.020$ and $cost = 8$ returning a $RMSE = 0.1127$

```{r svm-plot, cache=TRUE, message=FALSE, warning=FALSE, fig.height=4, fig.width=7}
plot(svm.tune)
```


## Model 2: Cubist

[Cubist references:
Background:  https://www.rulequest.com/cubist-win.html
Overview:  https://static1.squarespace.com/static/51156277e4b0b8b2ffe11c00/t/56e3056a3c44d8779a61988a/1457718645593/cubist_BRUG.pdf
Mechanics:  http://ftp.uni-bayreuth.de/math/statlib/R/CRAN/doc/vignettes/caret/caretTrain.pdf]

For a continuous response variable, a rule-based Cubist model functions like a piecewise linear model: each rule is a conjunction of conditions associated with a linear expression, and those rules can overlap with each other.  Adding to the interpretive complexity of those rules, Cubist models can also integrate an instance-based, nearest-neighbor approach that performs a composite prediction based on actual values of neighbors, predicted values of neighbors, and predicted values of observations of interest.  

Accordingly, hyper-parameters for Cubist models include:
- The number of rule-based models, or committees - these issue separate predictions that are averaged (5 recommended to balance computational cost with ensemble benefits)
- The number of neighbors over which to predict response values based on similar training observations

Based on cross-validation and a grid search across hyper-parameters, we found the best RMSE performance with an instance-based model that factoring in many neighbors built on non-pre-processed training data.

```{r, fig.height=2.5}
cub1_plot <- ggplot(cub_fit1)+
  theme_bw()+
  theme(legend.position = "none") + 
  labs(title = "Cubist 1", 
       y = "RMSE (CV)", 
       x = "") +
  theme(axis.text.x = element_text(angle = 90)) +
  scale_color_manual(values = c('#999999', '#57A0D3', '#95C8D8', '#008081', '#034D92', '#011232')) +
  scale_shape(guide = FALSE) +
  scale_x_continuous(labels = scales::number_format(accuracy = 1), breaks = c(0, 2, 4, 6, 8, 10)) +
  scale_y_continuous(labels = scales::number_format(accuracy = 0.005), limits = c(0.105, 0.160))

cub2_plot <- ggplot(cub_fit2)+
  theme_bw()+
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank()) + 
  labs(title = "Cubist 2", 
       y = "") +
  theme(axis.text.x = element_text(angle = 90)) +
  scale_color_manual(values = c('#999999', '#57A0D3', '#95C8D8', '#008081', '#034D92', '#011232')) +
  scale_shape(guide = FALSE) +
  scale_x_continuous(labels = scales::number_format(accuracy = 1), breaks = c(0, 2, 4, 6, 8, 10)) +
  scale_y_continuous(labels = scales::number_format(accuracy = 0.005), limits = c(0.105, 0.160))

cub3_plot <- ggplot(cub_fit3)+
  theme_bw()+
  theme(legend.justification = c(1,1), 
        legend.position = c(.98,.98), 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        legend.direction = 'horizontal', 
        legend.key.size = unit(.1, "cm"), 
        legend.box.background = element_rect(color = "#999999", 
                                             size = 1), 
        legend.text = element_text(color = "#999999"))+
  labs(title = "Cubist 3", x = "", y = "") +
  theme(axis.text.x = element_text(angle = 90)) +
  scale_color_manual(values = c('#999999', '#57A0D3', '#95C8D8', '#008081', '#034D92', '#011232')) +
  scale_shape(guide = FALSE) +
  scale_x_continuous(labels = scales::number_format(accuracy = 1), breaks = c(0, 2, 4, 6, 8, 10)) +
  scale_y_continuous(labels = scales::number_format(accuracy = 0.005), limits = c(0.105, 0.160))

ggarrange(cub1_plot, 
          cub2_plot, 
          cub3_plot, 
          ncol=3, 
          common.legend = TRUE, 
          legend = "bottom")
```


## Model 3: Multivariate Adaptive Regression Splines 

[BASED ON FINAL ALIGNMENT, COMBINE BELOW FROM JULIANN...]

MARS modeling was selected to assess the non-linear features in our data. This method uses a weighted sum to models non-linearities and interactions between variables. The model assesses cut-points between features that create the smallest error and prunes insignificant points to improve model accuracy.  

Our RMSE Cross Validation plots show us that pre-processing transformations did not have improve the MARS model. The model performed best on our training data when no transformations were applied.   

```{r, fig.height=2.5}
mars1_plot <- ggplot(mars_fit1)+
  theme_bw()+
  theme(legend.position = "none") + 
  labs(title="MARS1", y="RMSE (CV)", x="")+
  scale_color_manual(values=c("#95C8D8", "#008081", "#034D92"))+
  scale_shape(guide=FALSE)+
  scale_y_continuous(labels = scales::number_format(accuracy = 0.005),limits = c(0.118, 0.145))

mars2_plot <- ggplot(mars_fit2)+
  theme_bw()+
  theme(legend.position = "none", axis.title.y=element_blank(), 
        axis.text.y=element_blank()) + 
  labs(title="MARS1", y="")+
  scale_color_manual(values=c("#95C8D8", "#008081", "#034D92"))+
  scale_shape(guide=FALSE)+
  scale_y_continuous(labels = scales::number_format(accuracy = 0.005),limits = c(0.118, 0.145))

mars3_plot <- ggplot(mars_fit3)+
  theme_bw()+
  theme(legend.justification=c(1,1), 
        legend.position = c(.98,.98), 
        axis.title.y = element_blank(), 
        axis.text.y=element_blank(), 
        legend.direction = 'horizontal', 
        legend.key.size = unit(.1, "cm"), 
        legend.box.background = element_rect(color="#999999", size=1), 
        legend.text = element_text(color="#999999"))+
  labs(title="MARS3", x="", y="") +
  scale_color_manual(values = c("#95C8D8", "#008081", "#034D92"))+
  scale_shape(guide=FALSE)+
  scale_y_continuous(labels = scales::number_format(accuracy = 0.005),limits = c(0.118, 0.145))

ggarrange(mars1_plot, mars2_plot, mars3_plot, ncol=3, common.legend = TRUE, legend="bottom")

```

[...WITH BELOW FROM VINICIO]

By experience, MARS has been one of the better performing models. We should not spend too much time on trying to pick the best nnet model and pick MARS as our third. We anticipate MARS to outperform LM and NNET. 

```{r project2a15, echo=F}

# hyperparameter tuning for MARS
mars1 <- earth(PH ~ .,  data = df_train)

print(mars1)
```

Baseline MARS model performed better than any of our NNET models or best linear models. 

```{r project2a16, echo=F}
mars_imp <- varImp(mars1, scale = TRUE)

#mars_imp2<-as.data.frame(as.matrix(mars_imp$importance))

mars_imp%>% kable(caption="Linear Model 2 Variable Importance") %>% kable_styling()# %>% row_spec()

#plot(mars_imp2)
```

```{r project2a17, echo=F}
# hyperparameter tuning for MARS
mars2 <- earth(PH~BrandCode+PressureVacuum+AlchRel+Balling+Temperature+Usagecont+CarbPressure1+BowlSetpoint+HydPressure3+Density,  data = df_train)

print(mars2)
```


## Model 4: Random Forest

The optimal parameters for model was mtry = 31 and ntree = 2500. MAPE is `r s$MAPE` where as top 3 important predictors are `MnfFlow`, `BrandCode` and `PressureVacuum` for %incMSE and `MnfFlow`, `BrandCode` and `OxygenFiller` for IncNodePurity. Unlike `PLS`, `Random Forest` can produce 2 different variable importance plots. 

The first graph shows how much MSE would increase if a variable is assigned with values by random permutation. The second plot is based on `node purity` which is measured by the difference between RSS before and after the split on that variable (`Gini Index`). In short, each graph shows how much MSE or Impurity increases when each variable is randomly permuted.

```{r, cache=T, fig.align='center', warning=FALSE, fig.height=6, echo=F}
set.seed(58677)

# Algorithm Tune (tuneRF)
#bestmtry <- tuneRF(df_train[, -25], df_train[,25], stepFactor=1.5, improve=1e-5, ntree=2500)

##mtry <- ( (ncol(df_train) -1) / 3 ) or sqrt(ncol(df_train) - 1) # By default, # of predictors / 3 for regression, sqrt(# of predictors) for classification https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/tutorial-random-forest-parameter-tuning-r/tutorial/

# from above result, we got mtry= 27 and ntree=2500 as optimal parameters
rf_model2 <- randomForest(PH~., data=df_train, method="rf", mtry= 31, importance = TRUE, ntree = 2500)

# create MAPE table
train_rf_pred2 <- predict(rf_model2)

s <- data.frame(
RMSE = Metrics::rmse(df_train$PH, train_rf_pred2),
Rsquared = caret::R2(df_train$PH, train_rf_pred2),
MAPE = Metrics::mape(df_train$PH, train_rf_pred2) )

s%>%kable(caption="Model Summary - RF", booktabs=T)%>%kable_styling()%>%row_spec()

# plot varImp
Random_Forest_Variance_Importance <- rf_model2
varImpPlot(Random_Forest_Variance_Importance)
```

The optimal parameters for model was mtry = 31 and ntree = 2500. MAPE is `r s$MAPE` where as top 3 important predictors are `MnfFlow`, `BrandCode` and `PressureVacuum` for %incMSE and `MnfFlow`, `BrandCode` and `OxygenFiller` for IncNodePurity. Unlike `PLS`, `Random Forest` can produce 2 different variable importance plots. 

The first graph shows how much MSE would increase if a variable is assigned with values by random permutation. The second plot is based on `node purity` which is measured by the difference between RSS before and after the split on that variable (`Gini Index`). In short, each graph shows how much MSE or Impurity increases when each variable is randomly permuted.


# Model Performance 

*  Set1 = Caret: bagImputed; no additional pre-processing  
*  Set2 = Caret: bagImputed; PreP `method=c('center', 'scale', 'nzv', 'BoxCox')`


#### Train Performance:

```{r}
tbl.perf.train1 %>% 
  kable(caption="Train1 Performance", booktabs=T, digits=4) %>% 
  kable_styling() 

tbl.perf.train2 %>% 
  kable(caption="Train2 Performance", booktabs=T, digits=4) %>% 
  kable_styling() 
```


#### Test Accuracy:

```{r}
tbl.perf.test1 %>% 
  kable(caption="Test1 Performance", booktabs=T, digits=4) %>% 
  kable_styling() 

tbl.perf.test2 %>% 
  kable(caption="Test2 Performance", booktabs=T, digits=4) %>% 
  kable_styling() 

```

[UPDATE]

The ... model performed the best with the lowest accuracy scores. The ... model fit the data the best with the lowest variation between predicted and observed data for both the training and the test set.  

```{r}
Tbl_Accuracy %>% kable(digits=5,booktabs=T, caption="Accuracy Measures") %>% kable_styling() %>% column_spec(2:3, color = "#0074D9",bold = T)
```


#### Variable Importance

[UPDATE]

Variable importance varied greatly between the two models. The predictors, `MnfFlow`, `HydPressure3`, `Useagecont`, and `BowelSetpoint`, both ranked in the top 10 important variables for MARS and eNET model.

```{r, fig.height=2.5}
Plt_MARS_VarImp <- MARS_VarImp$importance %>% as.data.frame.array() %>% rownames_to_column("Variable") %>% top_n(10, Overall) %>% ggplot(aes(x=reorder(Variable, Overall), y=Overall)) + geom_point()+geom_segment(aes(x=Variable,xend=Variable,y=0,yend=Overall)) + coord_flip() + labs(y="Overall", x="", title="MARS") + theme_bw() + theme(axis.title.y = element_blank(), axis.title.x = element_blank())+scale_y_continuous(labels = scales::number_format(accuracy = 1,decimal.mark = '.')); 
Plt_eNET_VarImp <- eNET_VarImp$importance %>% as.data.frame.array() %>% rownames_to_column("Variable") %>% top_n(10, Overall) %>% ggplot(aes(x=reorder(Variable, Overall), y=Overall)) + geom_point() + geom_segment(aes(x=Variable,xend=Variable,y=0,yend=Overall)) + coord_flip() + labs(y="Overall", x="", title="eNET") + theme_bw() + theme(axis.title.y = element_blank(), axis.title.x = element_blank())+scale_y_continuous(labels = scales::number_format(accuracy = 1,decimal.mark = '.'));

grid.arrange(Plt_MARS_VarImp, Plt_eNET_VarImp, nrow=1, bottom = textGrob("Overall Variable Importance"))
```


# Conclusion

This section should contain final thoughts and save/discuss our student evaluation predictions.


# Appendix {-#Appendix}




